# -*- coding: utf-8 -*-
"""Copy of BiasDetectionIncome.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MleIze8iR-DCDaIAJidS0lZn3lO2_rQj
"""

rom google.colab import drive
import pandas as pd
# Mount Google Drive
drive.mount('/content/drive')

# Define file path (Change this to your actual file path in Google Drive)
file_path = "/content/drive/MyDrive/adult.data"  # Adjust path if needed
# Define column names manually
columns = ["age", "workclass", "fnlwgt", "education", "education_num", "marital_status",
           "occupation", "relationship", "race", "sex", "capital_gain", "capital_loss",
           "hours_per_week", "native_country", "income"]
# Load dataset with correct column names
df = pd.read_csv(file_path, names=columns, header=None, na_values=" ?")
# Display first few rows
df.head()

from google.colab import drive
drive.mount('/content/drive')

# Ensure native_country is treated as a string before applying strip()
df["native_country"] = df["native_country"].astype(str).apply(lambda x: 1 if x.strip() == "United-States" else 0)

# Convert sex to binary (1 = Male, 0 = Female)
df["sex"] = df["sex"].apply(lambda x: 1 if x.strip() == "Male" else 0)

# Convert income to binary (1 = >50K, 0 = <=50K)
df["income"] = df["income"].apply(lambda x: 1 if x.strip() == ">50K" else 0)

# One-hot encode relationship column
df = pd.get_dummies(df, columns=["relationship"], drop_first=True)

# One-hot encode remaining categorical features
categorical_features = ["workclass", "education", "marital_status", "occupation", "race"]
df = pd.get_dummies(df, columns=categorical_features, drop_first=True)

from sklearn.preprocessing import StandardScaler

# Define numerical features
numerical_features = ["age", "fnlwgt", "education_num", "capital_gain", "capital_loss", "hours_per_week"]

# Apply StandardScaler
scaler = StandardScaler()
df[numerical_features] = scaler.fit_transform(df[numerical_features])

print("Final Processed Dataset Shape:", df.shape)
df.head()

import seaborn as sns
import matplotlib.pyplot as plt

# Fix: Explicitly define bins to ensure only 0 and 1 are shown
sns.histplot(df, x="income", hue="sex", multiple="dodge", bins=[-0.5, 0.5, 1.5])

plt.title("Income Distribution by Gender (Fixed)")
plt.xlabel("Income (>50K: 1, <=50K: 0)")
plt.ylabel("Count")
plt.xticks(ticks=[0, 1], labels=["<=50K", ">50K"])  # Ensure correct labels
plt.show()

# Plot gender-based income disparity
income_gender = df.groupby("sex")["income"].mean() * 100
plt.figure(figsize=(6, 4))
sns.barplot(x=income_gender.index, y=income_gender.values, palette="coolwarm")
plt.title("Percentage of People Earning >50K by Gender")
plt.ylabel("Percentage")
plt.xticks(ticks=[0, 1], labels=["Female", "Male"])
plt.show()

"""#Disparate Impact (DI) calculation
DI is the ratio of the probability of favorable outcomes (earning >50K) for the unprivileged group to the privileged group.
"""

def disparate_impact(df, privileged_group, unprivileged_group, target_col="income"):
    p_privileged = df[df[privileged_group] == 1][target_col].mean()
    p_unprivileged = df[df[unprivileged_group] == 0][target_col].mean()
    return p_unprivileged / p_privileged

# Calculate Disparate Impact for Gender
di_gender = disparate_impact(df, "sex", "sex")

print(f"‚öñÔ∏è Disparate Impact (Gender): {di_gender:.2f}")

"""A DI of 0.36 means that females are 36% as likely as males to earn >50K.
A value below 0.8 is considered evidence of potential bias or discrimination (often called the "80% Rule").
Since 0.36 < 0.8, this indicates significant gender disparity in high-income earners.

#Statistical Parity Difference (SPD) calculation
SPD is the difference in the probability of favorable outcomes (earning >50K) between the unprivileged and privileged groups.
"""

def statistical_parity_difference(df, privileged_group, unprivileged_group, target_col="income"):
    p_privileged = df[df[privileged_group] == 1][target_col].mean()
    p_unprivileged = df[df[unprivileged_group] == 0][target_col].mean()
    return p_unprivileged - p_privileged

# Calculate Statistical Parity Difference for Gender
spd_gender = statistical_parity_difference(df, "sex", "sex")

print(f"‚öñÔ∏è Statistical Parity Difference (Gender): {spd_gender:.2f}")

"""A negative SPD (-0.20) means that females are 20% less likely than males to earn >50K.
A value of 0 would indicate perfect fairness, meaning equal income distribution across genders.
A highly negative value suggests that the unprivileged group (females) has significantly fewer favorable outcomes.

#Conclusion
üìä Comparing SPD vs. Disparate Impact (DI)
Metric	Formula	Ideal Value	Bias Threshold	Your Output
Statistical Parity Difference (SPD)	( P(Y=1	U) - P(Y=1	P) )	0 (No Bias)
Disparate Impact (DI)	( \frac{P(Y=1	U)}{P(Y=1	P)} )	1 (No Bias)
üí° Key Takeaways
‚úÖ SPD confirms a gender disparity in income distribution.
‚úÖ DI already indicated that women are only 36% as likely as men to earn >50K.
‚úÖ Both metrics together suggest a strong gender bias in income distribution.

# Training a Baseline Machine Learning Model (MLP Classifier)
Multi-layer Perceptron (MLP) Classifier
"""

from sklearn.model_selection import train_test_split

# Define features (X) and target (y)
X = df.drop(columns=["income"])  # Features
y = df["income"]  # Target (0 or 1)

# Split into training and test sets (70% Train, 30% Test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Display dataset shapes
print("Training Data Shape:", X_train.shape)
print("Test Data Shape:", X_test.shape)

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report

# Define MLP Classifier model
mlp_model = MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=1000, random_state=42)

# Train the model
mlp_model.fit(X_train, y_train)

# Predict on test data
y_pred = mlp_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"üß† Baseline MLP Model Accuracy: {accuracy:.2f}")

# Display classification report
print("\nüîç Classification Report:\n", classification_report(y_test, y_pred))

"""This means the model correctly classifies 82% of the test data.

"""

# Create a DataFrame for predictions
df_test = X_test.copy()
df_test["income_actual"] = y_test
df_test["income_predicted"] = y_pred

# Compute Bias Metrics After Model Predictions
di_after = disparate_impact(df_test, "sex", "sex", target_col="income_actual")
spd_after = statistical_parity_difference(df_test, "sex", "sex", target_col="income_actual")

print(f"‚öñÔ∏è Disparate Impact After Model Prediction: {di_after:.2f}")
print(f"‚öñÔ∏è Statistical Parity Difference After Model Prediction: {spd_after:.2f}")

# Remove gender from features
X_no_gender = X.drop(columns=["sex"])  # Drop gender column

# Split the new dataset (without gender)
X_train_ng, X_test_ng, y_train_ng, y_test_ng = train_test_split(X_no_gender, y, test_size=0.3, random_state=42)

# Train the model again without gender
mlp_model_ng = MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=1000, random_state=42)
mlp_model_ng.fit(X_train_ng, y_train_ng)

# Predict on test data
y_pred_ng = mlp_model_ng.predict(X_test_ng)

# Use the original test set (df_test) that contains "sex"
df_test_ng = X_test.copy()  # Restore original test set with "sex"
df_test_ng["income_actual"] = y_test_ng
df_test_ng["income_predicted"] = y_pred_ng

# Compute new bias metrics using original test set
di_after_ng = disparate_impact(df_test_ng, "sex", "sex", target_col="income_actual")
spd_after_ng = statistical_parity_difference(df_test_ng, "sex", "sex", target_col="income_actual")

print(f"‚öñÔ∏è Disparate Impact After Removing Gender: {di_after_ng:.2f}")
print(f"‚öñÔ∏è Statistical Parity Difference After Removing Gender: {spd_after_ng:.2f}")

# Compare Predictions Between the Original and Debiased Model
df_test_ng["income_predicted_original"] = y_pred  # From biased model
df_test_ng["income_predicted_debiased"] = y_pred_ng  # From debiased model

# Check if the predictions have changed
df_test_ng[["income_actual", "income_predicted_original", "income_predicted_debiased", "sex"]].head(20)

# Balance dataset (Equal number of males & females)
df_male = df[df["sex"] == 1]
df_female = df[df["sex"] == 0]

# Sample equal number of males and females
min_count = min(len(df_male), len(df_female))
df_balanced = pd.concat([df_male.sample(min_count, random_state=42), df_female.sample(min_count, random_state=42)])

# Train-test split
X_balanced = df_balanced.drop(columns=["income"])
y_balanced = df_balanced["income"]
X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42)

# Train model
mlp_model_bal = MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=1000, random_state=42)
mlp_model_bal.fit(X_train_bal, y_train_bal)

# Predict on test data
y_pred_bal = mlp_model_bal.predict(X_test_bal)

# Compute new bias metrics
df_test_bal = X_test_bal.copy()
df_test_bal["income_actual"] = y_test_bal
df_test_bal["income_predicted"] = y_pred_bal

di_after_bal = disparate_impact(df_test_bal, "sex", "sex", target_col="income_actual")
spd_after_bal = statistical_parity_difference(df_test_bal, "sex", "sex", target_col="income_actual")

print(f"‚öñÔ∏è Disparate Impact After Balancing Dataset: {di_after_bal:.2f}")
print(f"‚öñÔ∏è Statistical Parity Difference After Balancing Dataset: {spd_after_bal:.2f}")

!pip install shap lime

pip install lime

# SHAP Before Counterfactual Data Augmentation

import shap
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# üîπ Ensure SHAP works correctly
shap.initjs()

# ‚úÖ Convert dataset to numeric (avoid object types)
X_train_biased = X_train.apply(pd.to_numeric, errors='coerce').astype(np.float32)
X_test_biased = X_test.apply(pd.to_numeric, errors='coerce').astype(np.float32)

# ‚úÖ Handle NaN and Infinite values
X_train_biased = X_train_biased.fillna(0).replace([np.inf, -np.inf], 0)
X_test_biased = X_test_biased.fillna(0).replace([np.inf, -np.inf], 0)

# ‚úÖ Define a function to predict probabilities for SHAP
def model_predict(X):
    return mlp_model.predict_proba(X)  # Ensure it returns class probabilities

# Use KernelExplainer (better for deep learning models)
explainer = shap.KernelExplainer(model_predict, X_train_biased[:100])  # Using the first 100 samples for efficiency

# ‚úÖ Compute SHAP values for test data (limit to 50 for faster computation)
shap_values = explainer.shap_values(X_test_biased.iloc[:50])  # Limit to 50 for faster computation

# üìå Global Feature Importance (Summary Plot)
shap.summary_plot(shap_values, X_test_biased.iloc[:50])

# üìå Local Explanation for a Single Sample (force plot)
shap.plots.force(explainer.expected_value[0], shap_values[0], X_test_biased.iloc[0, :])


# LIME Before Counterfactual Data Augmentation

import lime
import lime.lime_tabular
import numpy as np
import tensorflow as tf

# Define the LIME explainer for tabular data
explainer_lime = lime.lime_tabular.LimeTabularExplainer(
    training_data=np.array(X_train_biased),  # Training data
    feature_names=X_train.columns.tolist(),  # Feature names
    class_names=["Income ‚â§50K", "Income >50K"],  # Class names
    mode="classification"  # Classification problem
)

# Select an instance from the test set for explanation
instance_idx = 10  # Choose any sample index
instance = X_test_biased.iloc[instance_idx].values.reshape(1, -1)  # Ensure correct shape

# ‚úÖ Convert model predictions into probabilities
def predict_fn(x):
    preds = mlp_model.predict(x)  # Get raw predictions
    if preds.shape[1] == 1:  # If it's a single-class output (binary classification)
        preds = tf.sigmoid(preds).numpy()  # Apply sigmoid to convert logits ‚Üí probabilities
        return np.hstack([1 - preds, preds])  # Ensure both class probabilities sum to 1
    return preds  # Return softmax probabilities (for multi-class)

# Generate LIME explanation for the selected instance
explanation_lime = explainer_lime.explain_instance(instance.flatten(), predict_fn, num_features=10)

# üìå Show explanation in text format
print(explanation_lime.as_list())

# üìä Visualize explanation
explanation_lime.show_in_notebook()

"""#Counterfactual Data Augmentation on Bias Metrics"""

# Create counterfactual copies with swapped gender
df_swapped = df.copy()
df_swapped["sex"] = 1 - df_swapped["sex"]  # Swap male (1) ‚Üî female (0)

# Combine original + counterfactual data
df_augmented = pd.concat([df, df_swapped])

# Train-test split
X_aug = df_augmented.drop(columns=["income"])
y_aug = df_augmented["income"]
X_train_aug, X_test_aug, y_train_aug, y_test_aug = train_test_split(X_aug, y_aug, test_size=0.3, random_state=42)

# Train model
mlp_model_aug = MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=1000, random_state=42)
mlp_model_aug.fit(X_train_aug, y_train_aug)

# Predict on test data
y_pred_aug = mlp_model_aug.predict(X_test_aug)

# Compute new bias metrics
df_test_aug = X_test_aug.copy()
df_test_aug["income_actual"] = y_test_aug
df_test_aug["income_predicted"] = y_pred_aug

di_after_aug = disparate_impact(df_test_aug, "sex", "sex", target_col="income_actual")
spd_after_aug = statistical_parity_difference(df_test_aug, "sex", "sex", target_col="income_actual")

print(f"‚öñÔ∏è Disparate Impact After Counterfactual Data Augmentation: {di_after_aug:.2f}")
print(f"‚öñÔ∏è Statistical Parity Difference After Counterfactual Data Augmentation: {spd_after_aug:.2f}")

# SHAP After Counterfactual Data Augmentation

import shap
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# üîπ Ensure SHAP works correctly
shap.initjs()

# ‚úÖ Convert dataset to numeric (avoid object types)
X_train_aug_biased = X_train_aug.apply(pd.to_numeric, errors='coerce').astype(np.float32)
X_test_aug_biased = X_test_aug.apply(pd.to_numeric, errors='coerce').astype(np.float32)

# ‚úÖ Handle NaN and Infinite values
X_train_aug_biased = X_train_aug_biased.fillna(0).replace([np.inf, -np.inf], 0)
X_test_aug_biased = X_test_aug_biased.fillna(0).replace([np.inf, -np.inf], 0)

# ‚úÖ Define a function to predict probabilities for SHAP
def model_predict_aug(X):
    return mlp_model_aug.predict_proba(X)  # Ensure it returns class probabilities

# Use KernelExplainer (better for deep learning models)
explainer_aug = shap.KernelExplainer(model_predict_aug, X_train_aug_biased[:100])  # Using the first 100 samples for efficiency

# ‚úÖ Compute SHAP values for test data after augmentation (limit to 50 for faster computation)
shap_values_aug = explainer_aug.shap_values(X_test_aug_biased.iloc[:50])  # Limit to 50 for faster computation

# üìå Global Feature Importance (Summary Plot) for augmented data
shap.summary_plot(shap_values_aug, X_test_aug_biased.iloc[:50])

# üìå Local Explanation for a Single Sample (force plot) for augmented data
shap.plots.force(explainer_aug.expected_value[0], shap_values_aug[0], X_test_aug_biased.iloc[0, :])


# LIME After Counterfactual Data Augmentation

import lime
import lime.lime_tabular
import numpy as np
import tensorflow as tf

# Define the LIME explainer for tabular data
explainer_lime_aug = lime.lime_tabular.LimeTabularExplainer(
    training_data=np.array(X_train_aug_biased),  # Training data
    feature_names=X_train_aug.columns.tolist(),  # Feature names
    class_names=["Income ‚â§50K", "Income >50K"],  # Class names
    mode="classification"  # Classification problem
)

# Select an instance from the test set for explanation
instance_idx_aug = 10  # Choose any sample index
instance_aug = X_test_aug_biased.iloc[instance_idx_aug].values.reshape(1, -1)  # Ensure correct shape

# ‚úÖ Convert model predictions into probabilities
def predict_fn_aug(x):
    preds = mlp_model_aug.predict(x)  # Get raw predictions
    if preds.shape[1] == 1:  # If it's a single-class output (binary classification)
        preds = tf.sigmoid(preds).numpy()  # Apply sigmoid to convert logits ‚Üí probabilities
        return np.hstack([1 - preds, preds])  # Ensure both class probabilities sum to 1
    return preds  # Return softmax probabilities (for multi-class)

# Generate LIME explanation for the selected instance (after augmentation)
explanation_lime_aug = explainer_lime_aug.explain_instance(instance_aug.flatten(), predict_fn_aug, num_features=10)

# üìå Show explanation in text format
print(explanation_lime_aug.as_list())

# üìä Visualize explanation for the augmented data
explanation_lime_aug.show_in_notebook()

from sklearn.metrics import accuracy_score, classification_report

# Compute model accuracy
accuracy_aug = accuracy_score(y_test_aug, y_pred_aug)
print(f"üöÄ Model Accuracy After Counterfactual Data Augmentation: {accuracy_aug:.4f}")

# Display detailed classification report
print("\nüîç Classification Report (After Counterfactual Augmentation):")
print(classification_report(y_test_aug, y_pred_aug))

# Compute accuracy of original biased model
accuracy_original = accuracy_score(y_test, y_pred)

# Compare performance
print("\nüìä Model Performance Before vs. After Bias Mitigation:")
print(f"Baseline Model Accuracy (With Bias): {accuracy_original:.4f}")
print(f"Debiased Model Accuracy (Counterfactual Augmentation): {accuracy_aug:.4f}")

"""‚úÖ The model‚Äôs accuracy improved from 82.48% ‚Üí 88.01%

This means that removing bias actually improved overall model performance instead of hurting it!
‚úÖ Better fairness without sacrificing performance

The Disparate Impact (DI) increased from 0.37 ‚Üí 1.03, meaning gender bias was reduced.
The Statistical Parity Difference (SPD) improved from -0.19 ‚Üí 0.01, making outcomes more balanced.
‚úÖ Model now generalizes better for different genders

Since counterfactual augmentation forces the model to not rely on gender, it learns more generalizable patterns that work for both men and women.
"""

# Part 8: Model-Based Bias Mitigation (Fair Learning Techniques)
# Since data-based bias mitigation (Counterfactual Data Augmentation) successfully reduced bias and improved accuracy, we will now explore Model-Based Bias Mitigation techniques.

# Model-based techniques modify the learning process itself to reduce bias without altering the dataset.

# üìå Model-Based Bias Mitigation Approaches
# We will implement three key methods to ensure fairness at the model level:

# üîπ 1Ô∏è‚É£ Train a Different Model (Decision Trees, Random Forests)
# Some ML models, like Decision Trees & Random Forests, are less prone to bias.
# We compare MLPClassifier with Decision Tree & Random Forest.
# üîπ 2Ô∏è‚É£ Add Fairness Constraints in Training
# Use Sample Weighting: Give higher weight to underrepresented groups (e.g., females earning >50K).
# Adjust loss function to penalize gender bias.
# üîπ 3Ô∏è‚É£ Post-Processing Fairness Correction
# Use Equalized Odds Post-processing: Adjust predictions after training to ensure fair outcomes.

"""##Trains a Decision Tree Model (DecisionTreeClassifier)

1Ô∏è‚É£ Trains a Decision Tree Model (DecisionTreeClassifier)

A Decision Tree is a simple, interpretable model that splits data at decision points (nodes).
The max_depth=5 limits tree depth to prevent overfitting.
It is trained on X_train_aug (counterfactually augmented dataset) and makes predictions on X_test_aug.
2Ô∏è‚É£ Trains a Random Forest Model (RandomForestClassifier)

A Random Forest is an ensemble of many Decision Trees, reducing overfitting and improving generalization.
n_estimators=100 means 100 trees are trained and their predictions are averaged.
3Ô∏è‚É£ Evaluates Model Performance Using Accuracy

accuracy_score(y_test_aug, y_pred_dt): Measures how well the Decision Tree model predicts the correct labels.
accuracy_score(y_test_aug, y_pred_rf): Measures the accuracy of the Random Forest model.
Both scores are printed.
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Train Decision Tree
dt_model = DecisionTreeClassifier(max_depth=5, random_state=42)
dt_model.fit(X_train_aug, y_train_aug)
y_pred_dt = dt_model.predict(X_test_aug)

# Train Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_aug, y_train_aug)
y_pred_rf = rf_model.predict(X_test_aug)

# Evaluate accuracy
acc_dt = accuracy_score(y_test_aug, y_pred_dt)
acc_rf = accuracy_score(y_test_aug, y_pred_rf)

print(f"üå≥ Decision Tree Accuracy: {acc_dt:.4f}")
print(f"üå≤ Random Forest Accuracy: {acc_rf:.4f}")

# Compute bias metrics for Random Forest
df_test_rf = X_test_aug.copy()
df_test_rf["income_actual"] = y_test_aug
df_test_rf["income_predicted"] = y_pred_rf  # Predictions from Random Forest

di_rf = disparate_impact(df_test_rf, "sex", "sex", target_col="income_actual")
spd_rf = statistical_parity_difference(df_test_rf, "sex", "sex", target_col="income_actual")

print(f"‚öñÔ∏è Disparate Impact for Random Forest: {di_rf:.2f}")
print(f"‚öñÔ∏è Statistical Parity Difference for Random Forest: {spd_rf:.2f}")

"""1Ô∏è‚É£ Bias has been effectively reduced after using counterfactual data augmentation.
2Ô∏è‚É£ The Random Forest model is fair, as both DI and SPD are within acceptable limits.
3Ô∏è‚É£ Compared to the original biased model, the new model treats males and females more equally in income predictions.

#Multi-Layer Perceptron (MLP) classifier
"""

# Use the original dataset (before counterfactual augmentation)
X_train_biased, X_test_biased, y_train_biased, y_test_biased = train_test_split(X, y, test_size=0.3, random_state=42)

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

# Train the biased MLP model
mlp_model_biased = MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=1000, random_state=42)
mlp_model_biased.fit(X_train_biased, y_train_biased)

# Predict on test data
y_pred_biased = mlp_model_biased.predict(X_test_biased)

# Compute accuracy
accuracy_biased = accuracy_score(y_test_biased, y_pred_biased)
print(f"üß† Biased MLP Model Accuracy: {accuracy_biased:.4f}")

# Compute bias metrics for the Biased Model
df_test_biased = X_test_biased.copy()
df_test_biased["income_actual"] = y_test_biased
df_test_biased["income_predicted"] = y_pred_biased

di_biased = disparate_impact(df_test_biased, "sex", "sex", target_col="income_actual")
spd_biased = statistical_parity_difference(df_test_biased, "sex", "sex", target_col="income_actual")

print(f"‚öñÔ∏è Biased Model - Disparate Impact: {di_biased:.2f}")
print(f"‚öñÔ∏è Biased Model - Statistical Parity Difference: {spd_biased:.2f}")

"""Interpretation of the Output
üî¥ ‚öñÔ∏è Biased Model - Disparate Impact: 0.35
A very low DI (0.35) means that the disadvantaged group (e.g., women) is 35% as likely to receive a positive outcome as the advantaged group (e.g., men).
Since DI < 0.8, this confirms significant bias.
üî¥ ‚öñÔ∏è Biased Model - Statistical Parity Difference: -0.20
The negative SPD (-0.20) indicates that the disadvantaged group gets 20% fewer positive predictions.
Ideally, SPD should be close to 0 for fair treatment across groups.
‚ö†Ô∏è Conclusion: The Biased Model is Discriminatory
The MLP classifier learned gender-based biases from the original dataset.
The model favors one group (likely males) over another (likely females) when predicting income.
These metrics confirm gender bias in income classification.

"""

# Applying Model-Based Debiasing Techniques
# Now that we've confirmed the model is biased, we will apply three model-based debiasing techniques:

# 1Ô∏è‚É£ Sample Weighting ‚Äì Adjust class weights during training.
# 2Ô∏è‚É£ Adversarial Debiasing ‚Äì Train a fairness adversary that detects bias.
# 3Ô∏è‚É£ Fairness-Penalized Loss Function ‚Äì Modify the loss function to penalize bias.

from sklearn.utils.class_weight import compute_sample_weight
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import accuracy_score

# Compute sample weights (higher weight for underrepresented classes)
sample_weights = compute_sample_weight(class_weight="balanced", y=y_train_biased)

# Train an MLP-like model using SGDClassifier with sample weighting
sgd_model_weighted = SGDClassifier(loss="log_loss", max_iter=1000, random_state=42)
sgd_model_weighted.fit(X_train_biased, y_train_biased, sample_weight=sample_weights)

# Predict
y_pred_weighted = sgd_model_weighted.predict(X_test_biased)

# Compute accuracy
accuracy_weighted = accuracy_score(y_test_biased, y_pred_weighted)
print(f"‚öñÔ∏è Fair Model Accuracy (with Sample Weighting): {accuracy_weighted:.4f}")

# Compute fairness metrics
df_test_weighted = X_test_biased.copy()
df_test_weighted["income_actual"] = y_test_biased
df_test_weighted["income_predicted"] = y_pred_weighted

di_weighted = disparate_impact(df_test_weighted, "sex", "sex", target_col="income_actual")
spd_weighted = statistical_parity_difference(df_test_weighted, "sex", "sex", target_col="income_actual")

print(f"‚öñÔ∏è Fair Model (Sample Weighting) - Disparate Impact: {di_weighted:.2f}")
print(f"‚öñÔ∏è Fair Model (Sample Weighting) - Statistical Parity Difference: {spd_weighted:.2f}")

"""‚úÖ What This Means:

 Sample weighting did not reduce bias, which means that gender discrimination is still present in the predictions.
 Accuracy dropped, meaning the model struggled to learn after reweighting the samples.
 We need stronger model-based debiasing techniques.

# 1Ô∏è‚É£ Train an Adversarial Model (Bias Detector)
 We train two models simultaneously:

 Main model (predicts income).
Adversary model (tries to predict gender from main model's predictions).
The main model learns to remove bias so that the adversary fails to detect gender.
"""

import tensorflow as tf
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import accuracy_score

# Ensure 'sex' column is aligned with the training set
y_train_sex = df.loc[X_train_biased.index, "sex"].values  # Correct indexing

# Convert to NumPy array (Keras requires NumPy arrays for labels)
y_train_sex = y_train_sex.astype(int)  # Ensure correct format

# Define the predictor model
input_layer = Input(shape=(X_train_biased.shape[1],))
hidden_layer = Dense(64, activation="relu")(input_layer)
hidden_layer = Dense(32, activation="relu")(hidden_layer)
output_layer = Dense(1, activation="sigmoid")(hidden_layer)

# Main model (Predicting income)
predictor_model = Model(inputs=input_layer, outputs=output_layer)

# Define the adversary (Detecting bias)
adversary_input = predictor_model(input_layer)
adversary_hidden = Dense(16, activation="relu")(adversary_input)
adversary_output = Dense(1, activation="sigmoid")(adversary_hidden)

# Adversary model (Detecting gender bias)
adversary_model = Model(inputs=input_layer, outputs=adversary_output)

# Train predictor model
predictor_model.compile(optimizer=Adam(), loss="binary_crossentropy", metrics=["accuracy"])
predictor_model.fit(X_train_biased, y_train_biased, epochs=10, batch_size=32, verbose=1)

# Freeze predictor and train adversary
for layer in predictor_model.layers:
    layer.trainable = False

adversary_model.compile(optimizer=Adam(), loss="binary_crossentropy", metrics=["accuracy"])
adversary_model.fit(X_train_biased, y_train_sex, epochs=10, batch_size=32, verbose=1)  # Use y_train_sex here

# Unfreeze predictor & continue training while minimizing bias
for layer in predictor_model.layers:
    layer.trainable = True

predictor_model.fit(X_train_biased, y_train_biased, epochs=10, batch_size=32, verbose=1)

# Predict and compute accuracy
y_pred_adversarial = predictor_model.predict(X_test_biased)
accuracy_adversarial = accuracy_score(y_test_biased, (y_pred_adversarial > 0.5).astype(int))
print(f"üß† Fair Model Accuracy (Adversarial Debiasing): {accuracy_adversarial:.4f}")

print(X_train_biased.shape, y_train_sex.shape)

# 1. Make predictions on test set
y_pred_adv_binary = (y_pred_adversarial > 0.5).astype(int).flatten()

# 2. Create DataFrame for fairness evaluation
df_test_adv = X_test_biased.copy()
df_test_adv["income_actual"] = y_test_biased
df_test_adv["income_predicted"] = y_pred_adv_binary
df_test_adv["sex"] = df.loc[X_test_biased.index, "sex"].values  # ensure 'sex' column is aligned

# 3. Define fairness metric functions

def disparate_impact(df, group_col, protected_value=0, target_col="income_predicted"):
    privileged = df[df[group_col] != protected_value][target_col]
    unprivileged = df[df[group_col] == protected_value][target_col]
    return (unprivileged.mean() / privileged.mean()) if privileged.mean() > 0 else 0

def statistical_parity_difference(df, group_col, protected_value=0, target_col="income_predicted"):
    privileged = df[df[group_col] != protected_value][target_col]
    unprivileged = df[df[group_col] == protected_value][target_col]
    return unprivileged.mean() - privileged.mean()

# 4. Calculate metrics
di_adv = disparate_impact(df_test_adv, group_col="sex", protected_value=0)
spd_adv = statistical_parity_difference(df_test_adv, group_col="sex", protected_value=0)

# 5. Display results
print(f"‚öñÔ∏è Disparate Impact (Adversarial Debiasing): {di_adv:.2f}")
print(f"‚öñÔ∏è Statistical Parity Difference (Adversarial Debiasing): {spd_adv:.2f}")

import tensorflow as tf
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
import numpy as np

# Convert training data to NumPy arrays (Keras prefers these)
X_train_np = X_train_biased.to_numpy().astype(np.float32)
y_train_np = y_train_biased.to_numpy().astype(np.float32)
sex_train_np = df.loc[X_train_biased.index, "sex"].to_numpy().astype(np.float32)

X_test_np = X_test_biased.to_numpy().astype(np.float32)
y_test_np = y_test_biased.to_numpy().astype(np.float32)

# Hyperparameters
BATCH_SIZE = 64
EPOCHS = 20
LAMBDA = 0.5  # weight for adversary loss

# Predictor model
input_layer = Input(shape=(X_train_np.shape[1],))
x = Dense(64, activation='relu')(input_layer)
x = Dense(32, activation='relu')(x)
pred_output = Dense(1, activation='sigmoid', name="income")(x)
predictor_model = Model(inputs=input_layer, outputs=pred_output)

# Adversary model (predicting gender from predictor's output)
adv_input = Input(shape=(1,))
a = Dense(16, activation='relu')(adv_input)
adv_output = Dense(1, activation='sigmoid', name="sex")(a)
adversary_model = Model(inputs=adv_input, outputs=adv_output)

# Optimizers
predictor_optimizer = Adam(learning_rate=1e-3)
adversary_optimizer = Adam(learning_rate=1e-3)

# Losses
bce = tf.keras.losses.BinaryCrossentropy()

# Training loop
for epoch in range(EPOCHS):
    print(f"üîÅ Epoch {epoch + 1}/{EPOCHS}")
    for i in range(0, len(X_train_np), BATCH_SIZE):
        X_batch = X_train_np[i:i+BATCH_SIZE]
        y_batch = y_train_np[i:i+BATCH_SIZE]
        sex_batch = sex_train_np[i:i+BATCH_SIZE]

        # === Train Adversary ===
        with tf.GradientTape() as tape_adv:
            # Predictor forward pass (frozen)
            y_pred_batch = predictor_model(X_batch, training=False)
            # Adversary predicts gender from income prediction
            sex_pred = adversary_model(y_pred_batch, training=True)
            loss_adv = bce(sex_batch, sex_pred)

        grads_adv = tape_adv.gradient(loss_adv, adversary_model.trainable_weights)
        adversary_optimizer.apply_gradients(zip(grads_adv, adversary_model.trainable_weights))

        # === Train Predictor to fool Adversary ===
        with tf.GradientTape() as tape_pred:
            y_pred_batch = predictor_model(X_batch, training=True)
            income_loss = bce(y_batch, y_pred_batch)
            sex_pred = adversary_model(y_pred_batch, training=False)
            adv_loss = bce(sex_batch, sex_pred)

            total_loss = income_loss - LAMBDA * adv_loss  # key trick

        grads_pred = tape_pred.gradient(total_loss, predictor_model.trainable_weights)
        predictor_optimizer.apply_gradients(zip(grads_pred, predictor_model.trainable_weights))

    print(f"‚úÖ Income Loss: {income_loss.numpy():.4f} | Adv Loss: {adv_loss.numpy():.4f}")

# Evaluate
y_pred_final = predictor_model(X_test_np).numpy()
y_pred_binary = (y_pred_final > 0.5).astype(int)
acc = accuracy_score(y_test_np, y_pred_binary)
print(f"\nüß† Final Adversarial Debiased Accuracy: {acc:.4f}")

# Count positive predictions by group
group_0_positive = df_test_adv[(df_test_adv["sex"] == 0) & (df_test_adv["income_predicted"] == 1)].shape[0]
group_1_positive = df_test_adv[(df_test_adv["sex"] == 1) & (df_test_adv["income_predicted"] == 1)].shape[0]

# Count total instances per group
group_0_total = df_test_adv[df_test_adv["sex"] == 0].shape[0]
group_1_total = df_test_adv[df_test_adv["sex"] == 1].shape[0]

# Compute rates
rate_0 = group_0_positive / group_0_total if group_0_total > 0 else 0
rate_1 = group_1_positive / group_1_total if group_1_total > 0 else 0

# Compute fairness metrics
disparate_impact = rate_0 / rate_1 if rate_1 > 0 else float('nan')
statistical_parity_diff = rate_0 - rate_1

print(f"‚öñÔ∏è Disparate Impact (Adversarial Debiasing): {disparate_impact:.2f}")
print(f"‚öñÔ∏è Statistical Parity Difference (Adversarial Debiasing): {statistical_parity_diff:.2f}")

import tensorflow as tf
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
import numpy as np

# Inputs
input_dim = X_train_biased.shape[1]
lambda_adv = 0.1  # Controls strength of fairness penalty

# Define predictor
inputs = Input(shape=(input_dim,))
x = Dense(64, activation="relu")(inputs)
x = Dense(32, activation="relu")(x)
predictor_output = Dense(1, activation="sigmoid")(x)

# Predictor model
predictor = Model(inputs=inputs, outputs=predictor_output)

# Adversary takes predictor output as input
# We need to stop gradient flow to predictor (using the output of predictor as input to adversary)
adversary_input = Input(shape=(1,))  # Create a new input for the adversary model
adv_x = Dense(16, activation="relu")(adversary_input)
adversary_output = Dense(1, activation="sigmoid")(adv_x)

# Adversary model
adversary = Model(inputs=adversary_input, outputs=adversary_output)

# Optimizers
predictor_optimizer = Adam()
adversary_optimizer = Adam()

# Loss functions
bce = tf.keras.losses.BinaryCrossentropy()

# Training Loop
batch_size = 64
epochs = 10
n_batches = int(np.ceil(len(X_train_biased) / batch_size))

X_train_np = X_train_biased.to_numpy()
y_income_np = y_train_biased.to_numpy().reshape(-1, 1)
y_gender_np = df.loc[X_train_biased.index, "sex"].values.reshape(-1, 1)

for epoch in range(epochs):
    print(f"üîÅ Epoch {epoch+1}/{epochs}")

    for batch in range(n_batches):
        start = batch * batch_size
        end = start + batch_size

        X_batch = X_train_np[start:end]
        y_income_batch = y_income_np[start:end]
        y_gender_batch = y_gender_np[start:end]

        # === 1. Train Adversary (freeze predictor) ===
        with tf.GradientTape() as tape_adv:
            pred_income = predictor(X_batch)  # Forward pass through predictor
            pred_gender = adversary(pred_income)  # Pass predictor output to adversary
            loss_adv = bce(y_gender_batch, pred_gender)

        grads_adv = tape_adv.gradient(loss_adv, adversary.trainable_weights)
        adversary_optimizer.apply_gradients(zip(grads_adv, adversary.trainable_weights))

        # === 2. Train Predictor (maximize adversary loss) ===
        with tf.GradientTape() as tape_pred:
            pred_income = predictor(X_batch)
            pred_gender = adversary(pred_income)
            loss_pred = bce(y_income_batch, pred_income)
            loss_fair = bce(y_gender_batch, pred_gender)
            total_loss = loss_pred - lambda_adv * loss_fair  # Adversarial fairness penalty

        grads_pred = tape_pred.gradient(total_loss, predictor.trainable_weights)
        predictor_optimizer.apply_gradients(zip(grads_pred, predictor.trainable_weights))

    print(f"‚úÖ Income Loss: {loss_pred:.4f} | Gender Loss: {loss_fair:.4f} | Total Loss: {total_loss:.4f}")

!pip install fairlearn

# Install fairlearn if it's not already installed
# !pip install fairlearn

import pandas as pd
from sklearn.model_selection import train_test_split
from fairlearn.metrics import MetricFrame
from sklearn.metrics import accuracy_score

# Assuming you have your data loaded in df and model trained as predictor
# If not, include those parts too

# Predict on test set
y_pred_final = predictor.predict(X_test_biased)
y_pred_final_binary = (y_pred_final > 0.5).astype(int)

# Create df_test for fairness metrics
df_test_adv = X_test_biased.copy()
df_test_adv["income_predicted"] = y_pred_final_binary
df_test_adv["sex"] = df.loc[X_test_biased.index, "sex"].values

# Ensure protected attribute is binary (0/1) if it isn't already
df_test_adv["sex"] = df_test_adv["sex"].astype(int)  # Convert if needed

# Calculate fairness metrics using MetricFrame
try:
    metric_frame = MetricFrame(
        metrics={"accuracy": accuracy_score},
        sensitive_features=df_test_adv["sex"],
        y_true=None,  # We're using predicted labels
        y_pred=df_test_adv["income_predicted"]
    )

    # Disparate Impact (DI)
    di = metric_frame.difference()
    print(f"‚öñÔ∏è Disparate Impact (Adversarial Debiasing): {di:.4f}")

    # Statistical Parity Difference (SPD)
    spd = metric_frame.statistical_parity_difference()
    print(f"‚öñÔ∏è Statistical Parity Difference (Adversarial Debiasing): {spd:.4f}")

except Exception as e:
    print(f"Error calculating fairness metrics: {str(e)}")
    # Fallback to manual calculation if library functions fail
    privileged = df_test_adv[df_test_adv["sex"] == 1]["income_predicted"].mean()
    unprivileged = df_test_adv[df_test_adv["sex"] == 0]["income_predicted"].mean()

    # Manual calculations for fairness metrics
    di_manual = unprivileged / privileged
    spd_manual = unprivileged - privileged

    print("\nUsing manual calculation:")
    print(f"‚öñÔ∏è Disparate Impact: {di_manual:.4f}")
    print(f"‚öñÔ∏è Statistical Parity Difference: {spd_manual:.4f}")

import pandas as pd
from sklearn.model_selection import train_test_split
from fairlearn.metrics import (
    disparate_impact,
    statistical_parity_difference,
    equalized_odds_difference
)

# Assuming you have your data loaded in df and model trained as predictor
# If not, include those parts too

# Predict on test set
y_pred_final = predictor.predict(X_test_biased)
y_pred_final_binary = (y_pred_final > 0.5).astype(int)

# Create df_test for fairness metrics
df_test_adv = X_test_biased.copy()
df_test_adv["income_predicted"] = y_pred_final_binary
df_test_adv["sex"] = df.loc[X_test_biased.index, "sex"].values

# Make sure protected attribute is binary (0/1) if it isn't already
df_test_adv["sex"] = df_test_adv["sex"].astype(int)  # Convert if needed

# Calculate fairness metrics
try:
    di = disparate_impact(
        df_test_adv,
        sensitive_feature="sex",
        y_true=None,  # We're using predicted labels
        y_pred="income_predicted"
    )

    spd = statistical_parity_difference(
        df_test_adv,
        sensitive_feature="sex",
        y_true=None,
        y_pred="income_predicted"
    )

    print(f"‚öñÔ∏è Disparate Impact (Adversarial Debiasing): {di:.4f}")
    print(f"‚öñÔ∏è Statistical Parity Difference (Adversarial Debiasing): {spd:.4f}")

except Exception as e:
    print(f"Error calculating fairness metrics: {str(e)}")
    # Fallback to manual calculation if library functions fail
    privileged = df_test_adv[df_test_adv["sex"] == 1]["income_predicted"].mean()
    unprivileged = df_test_adv[df_test_adv["sex"] == 0]["income_predicted"].mean()

    di_manual = unprivileged / privileged
    spd_manual = unprivileged - privileged

    print("\nUsing manual calculation:")
    print(f"‚öñÔ∏è Disparate Impact: {di_manual:.4f}")
    print(f"‚öñÔ∏è Statistical Parity Difference: {spd_manual:.4f}")

"""#SHAP"""

!pip install shap lime

import tensorflow as tf
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

input_layer = Input(shape=(X_train_biased.shape[1],))
hidden_layer = Dense(64, activation="relu")(input_layer)
hidden_layer = Dense(32, activation="relu")(hidden_layer)
output_layer = Dense(1, activation="sigmoid")(hidden_layer)

predictor_model = Model(inputs=input_layer, outputs=output_layer)
predictor_model.compile(optimizer=Adam(), loss="binary_crossentropy", metrics=["accuracy"])

predictor_model.fit(X_train_biased, y_train_biased, epochs=10, batch_size=32, verbose=1)

def combined_loss(y_true, y_pred):
    income_pred, gender_pred = y_pred[:, 0], y_pred[:, 1]

    # Loss for income prediction
    income_loss = tf.keras.losses.binary_crossentropy(y_true, income_pred)

    # Loss for gender prediction (higher is better for fairness)
    gender_loss = tf.keras.losses.binary_crossentropy(y_train_sex, gender_pred)

    # We minimize income loss and maximize gender loss to reduce bias
    return income_loss - gender_loss

X_test_biased = X_test_biased.apply(pd.to_numeric, errors='coerce')  # Convert to numeric
X_test_biased = X_test_biased.fillna(0)  # Replace NaNs with 0

import shap
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# üîπ Ensure SHAP works correctly
shap.initjs()

# ‚úÖ Convert dataset to numeric (avoid object types)
X_train_biased = X_train_biased.apply(pd.to_numeric, errors='coerce').astype(np.float32)
X_test_biased = X_test_biased.apply(pd.to_numeric, errors='coerce').astype(np.float32)

# ‚úÖ Handle NaN and Infinite values
X_train_biased = X_train_biased.fillna(0).replace([np.inf, -np.inf], 0)
X_test_biased = X_test_biased.fillna(0).replace([np.inf, -np.inf], 0)

# ‚úÖ Define a function to predict probabilities for SHAP
def model_predict(X):
    return predictor_model.predict(X).flatten()  # Ensure correct shape for SHAP

# ‚úÖ Use KernelExplainer (better for deep learning models)
explainer = shap.KernelExplainer(model_predict, shap.sample(X_train_biased, 100))  # Sample for efficiency

# ‚úÖ Compute SHAP values for test data
shap_values = explainer.shap_values(X_test_biased.iloc[:50])  # Limit to 50 for faster computation

# üìå Global Feature Importance (Summary Plot)
shap.summary_plot(shap_values, X_test_biased.iloc[:50])

# üìå Local Explanation for a Single Sample
shap.force_plot(explainer.expected_value, shap_values[0], X_test_biased.iloc[0, :])

explainer = shap.Explainer(predictor_model.predict, X_train_biased)

"""#LIME

"""

pip install lime

import lime
import lime.lime_tabular
import numpy as np
import tensorflow as tf

# Define the LIME explainer for tabular data
explainer = lime.lime_tabular.LimeTabularExplainer(
training_data=np.array(X_train_biased),  # Training data
feature_names=X_train_biased.columns.tolist(),  # Feature names
class_names=["Income ‚â§50K", "Income >50K"],  # Class names
mode="classification"  # Classification problem
)
# Select an instance from the test set for explanation
instance_idx = 10  # Choose any sample index
instance = X_test_biased.iloc[instance_idx].values.reshape(1, -1)  # Ensure correct shape

# ‚úÖ Convert model predictions into probabilities
def predict_fn(x):
    preds = predictor_model.predict(x)  # Get raw predictions
    if preds.shape[1] == 1:  # If it's a single-class output (binary classification)
        preds = tf.sigmoid(preds).numpy()  # Apply sigmoid to convert logits ‚Üí probabilities
        return np.hstack([1 - preds, preds])  # Ensure both class probabilities sum to 1
    return preds  # Return softmax probabilities (for multi-class)

# Generate LIME explanation for the selected instance
explanation = explainer.explain_instance(instance.flatten(), predict_fn, num_features=10)

# üìå Show explanation in text format
print(explanation.as_list())

# üìä Visualize explanation
explanation.show_in_notebook()

