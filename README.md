# Breaking-Bias-AI-Bias-Detection-and-Mitigation

## Overview

Developed a machine learning pipeline to detect and mitigate bias in income prediction models, focusing on ensuring fairness across sensitive attributes like gender and race. Leveraged Python, Jupyter Notebook, Pandas, and Scikit-learn for data preprocessing, exploratory data analysis, and model development. Implemented fairness-aware algorithms such as adversarial debiasing, fairness-penalized optimization, and threshold tuning. Evaluated model performance using accuracy as well as fairness metrics including Demographic Parity and Equal Opportunity Difference.

### Abstract

This project investigates algorithmic bias in predictive modeling, using the UCI Adult Income dataset as a case study. The goal is to identify patterns of unfair treatment in model predictions and apply strategies to reduce disparate outcomes. By incorporating statistical fairness metrics, adversarial training techniques, and post-processing methods like threshold optimization, the project demonstrates how machine learning systems can be tuned not only for accuracy but also for ethical accountability. The results show measurable improvements in fairness metrics with minimal sacrifice in model performance, highlighting the feasibility of equitable AI design.

## Contributions

Contributions to this project are welcome. Feel free to open a pull request or an issue if you have any suggestions or bug reports.

## Members 

- [Aditya Singh](https://github.com/adityasingh151)
